rsync -avz /proj/ClusterSched16/719 /groups/ClusterSched16/advcc25/

ssh m.team25.ClusterSched16.nome.nx
export PATH="/groups/ClusterSched16/advcc25/719/bin:$PATH"

# master node
rm -f ~/tetris.deth
printf    "m.team25.ClusterSched16.nome.nx\n"     >> ~/tetris.deth
printf    "s-1.team25.ClusterSched16.nome.nx\n"    >> ~/tetris.deth
printf    "s-2.team25.ClusterSched16.nome.nx\n"    >> ~/tetris.deth
printf    "s-3.team25.ClusterSched16.nome.nx\n"    >> ~/tetris.deth
printf    "s-4.team25.ClusterSched16.nome.nx\n"    >> ~/tetris.deth
cd ~ ; tetris.sh

# master node
rsync /proj/ClusterSched16/images/debian-current.qcow.GOLD /groups/ClusterSched16/advcc25/debian-current.qcow
vi /groups/ClusterSched16/advcc25/719/bin/gennetmaster.py   #rackcap = [1,4,6,6,6]
cd /groups/ClusterSched16/advcc25/719/bin
sudo ./net-master.sh 25

# slave node
ssh s-1.team25.ClusterSched16.nome.nx "cd /groups/ClusterSched16/advcc25/719/bin; sudo ./net-slave.sh"
ssh s-2.team25.ClusterSched16.nome.nx "cd /groups/ClusterSched16/advcc25/719/bin; sudo ./net-slave.sh"
ssh s-3.team25.ClusterSched16.nome.nx "cd /groups/ClusterSched16/advcc25/719/bin; sudo ./net-slave.sh"
ssh s-4.team25.ClusterSched16.nome.nx "cd /groups/ClusterSched16/advcc25/719/bin; sudo ./net-slave.sh"

# master node
ssh m.team25.ClusterSched16.nome.nx
cd /groups/ClusterSched16/advcc25/719/bin
sudo ./quickstart.sh 25 &
ssh 10.25.1.10
addgroup --gid 6043 --force-badname Clu-advcc25
adduser --uid 10156 --gid 6043 yunfany
mkdir .ssh
cd .ssh
cat .ssh/id_dsa.pub | ssh root@10.25.1.10 'cat >> .ssh/authorized_keys'

#10.25.1.10
sudo chmod 777 /etc/sudoers
vi /etc/sudoers
sudo chmod 440 /etc/sudoers
sudo chmod 777 /etc/fstab
vi /etc/fstab
sudo chmod 664 /etc/fstab

# !! 
sudo su xiaoguaz # same to yunfany
mkdir .ssh
ssh-keygen -t dsa
cat ~/.ssh/id_dsa.pub >> ~/.ssh/authorized_keys
### and put the public key of master into authorized_keys


/sbin/shutdown -h now

# master node
ssh m.team25.ClusterSched16.nome.nx "cd /groups/ClusterSched16/advcc25/719/bin; sudo ./rackstart.sh r0 16G 8"

ssh 10.25.1.10
exit

# setup: slave node in master node
ssh s-1.team25.ClusterSched16.nome.nx "cd /groups/ClusterSched16/advcc25/719/bin; sudo ./rackstart.sh r1 8G 4" &
ssh s-2.team25.ClusterSched16.nome.nx "cd /groups/ClusterSched16/advcc25/719/bin; sudo ./rackstart.sh r2 4G 2" &
ssh s-3.team25.ClusterSched16.nome.nx "cd /groups/ClusterSched16/advcc25/719/bin; sudo ./rackstart.sh r3 4G 2" &
ssh s-4.team25.ClusterSched16.nome.nx "cd /groups/ClusterSched16/advcc25/719/bin; sudo ./rackstart.sh r4 4G 2" &

cd /groups/ClusterSched16/advcc25/719/bin
export PATH=”/groups/ClusterSched16/advcc25/719/bin:$PATH”
mpssh -s -f vmips date
mpssh -s -f vmips "touch /var/tmp/foo"
mpssh -s -f vmips "ls -l /var/tmp/foo"

# !sudo
sudo su
apt-get install nfs-kernel-server
mkdir -p /opt/projects/advcc
chmod 777 /etc/exports
echo "/opt/projects/advcc 10.25.*(rw,sync,no_root_squash)" >> /etc/exports
chmod 444 /etc/exports
service nfs-kernel-server start
exit
# done sudo

cd /groups/ClusterSched16/advcc25/719/bin
mpssh -s -f vmips "sudo reboot"

# master
sudo chown -R xiaoguaz /opt/projects   # if needed
cd /opt/projects/advcc
cp /proj/ClusterSched16/hadoop-2.2-advcc.tar.gz .
tar -xvf hadoop-2.2-advcc.tar.gz

# for phase 2
cp -rf ~/etc_p2 /opt/projects/advcc/hadoop/hadoop-2.2.0/
rm -rf  /opt/projects/advcc/hadoop/hadoop-2.2.0/etc
cd /opt/projects/advcc/hadoop/hadoop-2.2.0
mv etc_p2 etc



cd /opt/projects/advcc/hadoop/hadoop-2.2.0/
cp /proj/ClusterSched16/tarfiles/thrift-advcc-example.tgz .
cp /proj/ClusterSched16/tarfiles/exp-advcc.phase2.tgz .
tar xvf exp-advcc.phase2.tgz
tar xvf thrift-advcc-example.tgz

cd /groups/ClusterSched16/advcc25/719/bin
mpssh -s -f vmips "sudo chown -R xiaoguaz /mnt"    # if needed?
ssh 10.25.1.10 "rm -f start_yarn.sh"

ssh 10.25.1.10 "printf 'export HADOOP_HOME=/opt/projects/advcc/hadoop/hadoop-2.2.0; export HADOOP_CONF_DIR=\${HADOOP_HOME}/etc/hadoop; export HADOOP_PREFIX=\${HADOOP_HOME}; export HADOOP_YARN_HOME=\${HADOOP_HOME}; export JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64; \${HADOOP_PREFIX}/bin/hdfs namenode -format coolgarth; \${HADOOP_PREFIX}/sbin/hadoop-daemon.sh --config \${HADOOP_CONF_DIR} --script hdfs start namenode; \${HADOOP_YARN_HOME}/sbin/yarn-daemon.sh --config \${HADOOP_CONF_DIR} start resourcemanager; \${HADOOP_YARN_HOME}/sbin/yarn-daemon.sh start proxyserver --config \${HADOOP_CONF_DIR}; \${HADOOP_PREFIX}/sbin/mr-jobhistory-daemon.sh start historyserver --config \${HADOOP_CONF_DIR}' >> start_yarn.sh"

ssh 10.25.1.10 "bash start_yarn.sh"

cd /groups/ClusterSched16/advcc25/719/bin
mpssh -s -f vmips "rm -f start_yarn2.sh"
mpssh -s -f vmips "printf 'export HADOOP_HOME=/opt/projects/advcc/hadoop/hadoop-2.2.0; export HADOOP_CONF_DIR=\${HADOOP_HOME}/etc/hadoop; export HADOOP_PREFIX=\${HADOOP_HOME}; export HADOOP_YARN_HOME=\${HADOOP_HOME}; export JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64; /opt/projects/advcc/hadoop/hadoop-2.2.0/sbin/hadoop-daemon.sh --config \${HADOOP_CONF_DIR} --script hdfs start datanode; /opt/projects/advcc/hadoop/hadoop-2.2.0/sbin/yarn-daemon.sh --config \${HADOOP_CONF_DIR} start nodemanager' >> start_yarn2.sh"

ssh 10.25.1.10 "rm -f start_yarn2.sh"

ssh 10.25.1.10 "printf 'export HADOOP_HOME=/opt/projects/advcc/hadoop/hadoop-2.2.0; export HADOOP_CONF_DIR=\${HADOOP_HOME}/etc/hadoop; export HADOOP_PREFIX=\${HADOOP_HOME}; export HADOOP_YARN_HOME=\${HADOOP_HOME}; export JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64; \${HADOOP_HOME}/sbin/yarn-daemon.sh  --config \${HADOOP_CONF_DIR}/amnode start nodemanager' >> start_yarn2.sh"

mpssh -s -f vmips "bash start_yarn2.sh"

mpssh -s -f vmips "printf 'HADOOP_HOME=/opt/projects/advcc/hadoop/hadoop-2.2.0\nHADOOP_CONF_DIR=\${HADOOP_HOME}/etc/hadoop' >> .bashrc"



# mpssh -u root -s -f vmips "rm -f start_yarn2.sh"

# 10.25.1.10
ssh-keyscan r0h0 >> ~/.ssh/known_hosts
for r in 1 2 3 4
do  
    for h in 0 1 2 3 4 5
    do 
        ssh-keyscan r${r}h${h} >> ~/.ssh/known_hosts
    done
done
for r in 1 2 3 4
do  
    for h in 0 1 2 3 4 5
    do 
        scp ~/.ssh/known_hosts r${r}h${h}:~/.ssh/known_hosts
    done
done

export LD_LIBRARY_PATH=/usr/local/lib/
export HADOOP_HOME=/opt/projects/advcc/hadoop/hadoop-2.2.0; export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop; export HADOOP_PREFIX=$HADOOP_HOME; export HADOOP_YARN_HOME=$HADOOP_HOME; export JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64;


watch -n1 "$HADOOP_HOME/bin/yarn node -list -all |sort"
/opt/projects/advcc/hadoop/hadoop-2.2.0/bin/hdfs dfsadmin -report


export HADOOP_HOME=/opt/projects/advcc/hadoop/hadoop-2.2.0; export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop; export HADOOP_PREFIX=${HADOOP_HOME}; export HADOOP_YARN_HOME=${HADOOP_HOME}; export JAVA_HOME=/usr/lib/jvm/java-6-openjdk-amd64

$HADOOP_HOME/bin/hdfs dfsadmin -report
cd /opt/projects/advcc/hadoop/hadoop-2.2.0/
$HADOOP_HOME/bin/yarn jar hadoop-mapreduce-examples-2.2.0.jar pi 16 1000

cd /opt/projects/advcc/hadoop/hadoop-2.2.0/exp
./replayTrace.py -c config-mini -t traceMPI-mini # if we have phase1 file



${HADOOP_YARN_HOME}/sbin/yarn-daemon.sh --config ${HADOOP_CONF_DIR} stop resourcemanager
${HADOOP_YARN_HOME}/sbin/yarn-daemon.sh --config ${HADOOP_CONF_DIR} start resourcemanager


cp $HADOOP_CONF_DIR/yarn-site.xml.TetriScheduler $HADOOP_CONF_DIR/yarn-site.xml


# Run thrift code stub generator:
thrift -out . --gen java tetrisched.thrift
thrift -out . --gen cpp tetrisched.thrift

# Setup library path:
export LD_LIBRARY_PATH=/usr/local/lib/

# Build java server:
javac -cp lib/libthrift-0.9.1.jar:lib/slf4j-api-1.7.7.jar Server.java YARNTetrischedHandler.java tetrisched/YARNTetrischedService.java

# Build java client:
javac -cp lib/libthrift-0.9.1.jar:lib/slf4j-api-1.7.7.jar Client.java tetrisched/TetrischedService.java tetrisched/job_t.java

# Build c++ server and client:
make

# Run cpp server:
cp schedpolserver ../exp/

./replayTrace.py -c config-mini -t XXX

